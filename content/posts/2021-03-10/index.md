---
title: Test Case0
author: Karter
date: 2021-03-10
hero: ./images/hero.jpg
---

BERT 저자진은 두 문장 간 유사도를 계산하는 Semantic Textual Similarity, 두 문장 사이 추론 관계를 분류하는 Natural Language Inference 등과 같이 두 문장을 입력으로 받아 하나의 값을 반환하는 문제 세팅에서 [SEP] 토큰을 활용한 Cross-encoder 방식을 채택하였습니다.

트랜스포머 아키텍처를 활용한 Cross-encoder 방식의 장점은 두 문장이 Self-attention을 통해 상호 간 attend 될 수 있으므로 두 문장 사이의 관계를 면밀하게 파악할 수 있다는 점에 있습니다. 두 문장을 구성하는 모든 토큰들 간 어텐션 연산이 수행되기 때문이죠.

그러나 Cross-encoder는 치명적인 단점을 지니고 있습니다. 바로 문장 간 유사도를 활용한 검색, 임베딩을 활용한 클러스터링 등에 있어 연산량의 한계가 존재한다는 점입니다. 검색 엔진에 10,000개 문서를 대상으로 한 서치 쿼리를 보낸다고 생각해봅시다. 서치 쿼리와 유사한 Top-k 문서를 찾기 위해 우리는 "[CLS] 서치 쿼리 [SEP] n-th 문서 [SEP]"의 임베딩을 10,000 번의 

Cross-encoder 연산을 통해 구해야합니다. 입력 받은 서치 쿼리에 따라 시퀀스 임베딩이 변하기에 매번 새로운 연산을 수행해야 하는 것이죠. 
이러한 연산량의 한계를 지적하며 등장한 논문이 2019년 UKP Lab에서 내놓았던 Sentence BERT (SBERT) 입니다. SBERT는 Siamese Network를 따라 모든 문장을 Cross-encoder 형식이 아닌 단일 인코더 형식으로 연산해 각 문장의 임베딩을 기준으로 유사도를 구하는 아키텍처를 제시하였습니다. [그림 1]

이렇게 되면 앞서 언급한 연산량 문제가 어떻게 해결될 수 있을까요? 우선 검색 대상이 되는 10,000 개의 문서는 SBERT에 의해 Pre-computed 되어 저장될 수 있게 됩니다. Cross-encoder 방식에서는 매번 달라지는 서치 쿼리와 새로운 연산을 통해 유사도를 구해야하므로 미리 계산을 해놓을 수 없지만, SBERT 방식에서는 각 문장의 임베딩 연산이 독립적으로 수행되고 이렇게 해서 얻어진 임베딩을 기준으로 유사도만 구하면 되기 때문입니다.

즉, 검색 후보가 되는 문서들에 대해서는 미리 임베딩 계산을 해놓은 후, 새로이 들어온 서치 쿼리의 임베딩만 얻은 후 문서들의 임베딩과 유사도만 구하면 되는 문제로 치환할 수 있게 됩니다. 이렇게 되면 많은 연산량이 필요한 BERT 임베딩을 10,000 번에서 1번으로 줄일 수 있을 뿐더러, FAISS와 같은 Approximate Nearest Neighbor Search 패키지를 활용해 유사도를 구하는 연산도 적은 컴퓨팅으로 수행할 수 있게 됩니다.

SBERT에서 차용한 해당 구조를 Bi-encoder라고 이야기합니다. 예제에서 말씀드린 바와 같이 Computation-heavy 한 Cross-encoder 구조는 상용단에서 활용이 불가능하기에, Bi-encoder가 현실적 대안인 것은 사실입니다. 그러나 많은 연구에서 Bi-encoder의 태스크 성능이 Cross-encoder의 성능보다 떨어짐을 실험을 통해 보여주었습니다. 전산학에서의 성능 vs. 속도의 트레이드 오프가 이곳에서도 발생하는 것이죠 !

특히 이러한 성능 차이는 Fine-tuning을 할 수 있는 데이터셋이 부족할 때 더 심하게 나타나게 되는데 STS, NLI 등의 페어 데이터셋은 어노테이션을 하는데 많은 비용이 들기 때문에 현실적으로 쉽게 수급할 수도 없다는데에서 개선의 한계가 나타나게 됩니다. [그림 2]

해당 문제를 누구보다 잘 알고 있었을 사람들은 당연히 SBERT를 고안한 랩분들이겠죠? 서론이 길었습니다 ! 이번 NAACL 2021에 억셉된 Augmented SBERT는 앞서 언급한 문제로 인해 현실적으로 차용하고 있는 Bi-encoder 구조의 SBERT 성능 이슈를 개선하기 위해 Un-labeled 데이터셋에서 Silver 데이터셋을 추출해 Gold 데이터셋과 혼합하여 Fine-tuning 하였을 때, 성능 개선을 도모할 수 있다는 주장을 제기한 논문입니다.

가장 중요하게 다루어져야 할 Un-labeled 데이터셋에서 Silver 데이터셋을 추출하는 기법에 대해 살펴봅시다. Un-labeled 데이터를 Silver 데이터로 라벨링하는데 활용되는 모듈은 Fine-tuning 된 BERT (Not SBERT) 입니다. [그림 3]을 보시면 Unlabeled 데이터가 Cross-encoder를 거쳐 Silver 데이터가 되는 것을 확인할 수 있습니다.

즉, Un-labeled 페어 데이터를 성능이 좋은 Cross-encoder 형태의 BERT로 Weakly-labeled 데이터로 변환 후, 이를 Gold 데이터셋에 증강해 훈련에 활용하는 구조라고 이해하시면 됩니다. 굉장히 간단하지만 실험 배경과 가정에 의거하면 설득력이 있는 세팅입니다.

추출의 대상이 되는 Un-labeled 데이터셋은 임의의 문장 두 쌍으로 구성된 아무 데이터셋으로 설정할 수도 있지만, 이미 어노테이션 된 데이터셋에서 어노테이션 되지 않은 조합을 추출하여 구성된 데이터셋 역시 Un-labeled 데이터셋으로 설정할 수 있을 것입니다 (1번의 A 문장과 2번의 B 문장 간 라벨링은 되어 있지 않으므로). 논문에서는 후자의 방법을 채택합니다.

이렇게 되면 전체 n 개 훈련 데이터 중 n x (n - 1) / 2개의 조합이 발생하게 되는데 이렇게 얻어진 모든 조합을 Silver 데이터로 변환하는 것은 또 연산량이 너무 많이 드는 작업입니다. 따라서 논문에서는 전체 Un-labeled 데이터를 라벨링 하는 것이 아닌, 여러 샘플링 기법을 활용합니다.

첫 번째 샘플링에서는 전체 조합 중 임의로 추출된 쌍들 중 긍정으로 분류된 데이터는 남겨두고 부정으로 분류된 데이터를 임의로 제거해나가는 기법을 활용합니다. 이렇게 하는 이유는 일반적으로 샘플링 후, Cross-encoder로 연산을 수행했을 때 긍정으로 분류될 데이터보다 부정으로 분류될 데이터가 훨씬 많을 것이기에 그 비율을 맞추어 주기 위해서입니다. 분류가 아닌 회귀 태스크의 경우, Gold 데이터들의 값 분포를 최대한 맞추는 방식으로 샘플링을 수행합니다.

두 번째는 나이브한 방법으로 BM 25를 활용한 샘플링입니다. Un-labeled 데이터셋에서 문장 하나를 뽑아 다른 문장들을 대상으로 BM 25 알고리즘을 적용해 Top-k 개 문장을 추출하고, 추출된 문장들을 페어로 설정해 Cross-encoder 연산을 통해 Weakly-labeled 데이터로 활용합니다.

세 번째는 Fine-tuned SBERT를 활용하는 샘플링입니다. BM 25를 활용한 샘플링은 Lexical overlapped 문장들을 대상으로만 라벨링이 수행되기 때문에 Lexcially-overlapped 되지는 않았지만 의미론적으로 유사한 문장들 간 라벨링은 전혀 수행되지 않는다는 단점을 지니게 됩니다. 이러한 문제를 해결하기 위해 기학습된 SBERT를 활용해 시맨틱 유사도가 높은 Top-k 개 문장을 추출해, 추출된 문장들을 페어로 설정해 Weakly-labeled 데이터로 활용합니다.

*cf. 논문에는 의미적으로 유사한 Top-k 문장을 추출해 Silver 데이터셋으로 활용하였다고 명시되어 있는데, 이를 모두 긍정쌍으로 활용했다는 것인지 Cross-encoder의 분류 결과에 따라 활용했다는 것인지에 대해서는 상세되어 있지 않습니다.*

이제 [그림 4]에서 Augmented SBERT의 결과를 확인하실 수 있습니다. 실험에서는 Cross-encoder를 활용하는 BERT가 실험의 Upper-bound, Augmentation이 수행되지 않은 Vanilla SBERT가 실험의 Lower-bound로 설정되었습니다. 그리고 아래 4개 열을 보게 되면 샘플링 기법별 성능이 기록되어 있습니다.

긍/부정 비율과 Regression score density를 맞추어준 KDE 샘플링과 BM 25를 활용한 샘플링이 좋은 성능을 보였군요. Vanilla SBERT 보다 작게는 1, 크게는 6 까지 성능 개선을 이끌어 낼 수 있었습니다. BM 25 샘플링이 Fine-tuned SBERT를 활용하여 Silver 데이터셋을 구축한 샘플링 보다 좋은 성능을 기록한 것은 의외의 결과네요.

어찌보면 간단한 실험이기도 하지만 잘 설계된 실험 가정을 따라 결과를 마주하게 되니 "여기서 왜 이러한 개선이 생기지?"와 같은 의구심은 크게 들지 않았던, 재밌게 읽을 수 있었던 논문이었습니다. 세 번째 샘플링이 좋은 성능을 보이지 못한 점은 의아하긴 하지만, 논문에서 상세를 하지 않은 것을 보면 모종의 이유가 있으리라 생각이 됩니다... 다른 부분들이 좋으니 패쓰...

논문에서는 위 실험 외에도 Out-of-domain으로 추론 환경이 옮겨 갔을 때 성능이 급격하게 하락하는 SBERT의 문제를 개선하기 위해 In & Cross 도메인에 대한 실험도 수행하였으며, 이러한 Domain Adaptation에 있어서는 앞서 설명한 In Domain 세팅보다 훨씬 더 큰 개선을 이끌어 낼 수 있음을 보여주기도 하였습니다.

오늘은 실험 배경까지 설명하느라 글이 길어진 감이 있네요 😇 연구를 Fast-following 하려고 노력하는 입장에서 다양한 기관을 지켜보면 UKP Lab은 항상 실용적인 연구를 멋지게 수행해낸다는 인상이 강하게 있는 것 같습니다. 많은 분들이 이미 UKP Lab에서 내놓은 Sentence Transformers 라이브러리를 활용하고 계실텐데 오늘 소개드린 Augmented SBERT 역시 해당 라이브러리를 통해 실험해보실 수 있다고 합니다.

글이 길어져 디테일을 많이 담지 못했습니다. 더 자세한 내용을 살펴보시고 싶으신 분들은 원문을 참조해주세요 !
